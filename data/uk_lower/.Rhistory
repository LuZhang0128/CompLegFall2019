sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo"), pkgTest)
gssexam %>%
group_by(relig) %>%
max(count(relig))
##################
#b) Recode the variable *relig* to a new dichotomous variable *religion* with two categories:
#the one is the category with the most frequency named as itself,
#the other one is the rest of categories named "Others".
##################
gssexam <- read.csv("~/Desktop/gssexam.csv")
gssexam %>%
group_by(relig) %>%
max(count(relig))
gssexam %>%
group_by(relig) %>%
max(count())
gssexam %>%
group_by(relig)
gssexam %>%
group_by(relig) %>%
count()
gssexam %>%
select(relig) %>%
group_by(relig) %>%
count()
gssexam %>%
select(relig) %>%
group_by(relig) %>%
max(count())
gssexam %>%
select(relig) %>%
group_by(relig) %>%
count() %>%
max()
freq <- gssexam %>%
select(relig) %>%
group_by(relig) %>%
count()
max(freq)
View(freq)
max(freq$freq)
gssexam <- read.csv("~/Desktop/gssexam.csv")
f <- gssexam %>%
select(relig) %>%
group_by(relig) %>%
count()
max(f$freq)
f$relig[which.max(f$freq)]
?mutate
gssexam$religion <- c("Protestant","Others")
level(gssexam$religion) <- c("Protestant","Others")
levels(gssexam$religion) <- c("Protestant","Others")
gssexam$religion <- "initial"
gssexam$religion[gssexam$relig == "Protestant"] <- "Protestant"
gssexam$religion <- NA
gssexam$religion[gssexam$relig == "Protestant"] <- "Protestant"
gssexam <- read.csv("~/Desktop/gssexam.csv")
f <- gssexam %>%
select(relig) %>%
group_by(relig) %>%
count()
f$relig[which.max(f$freq)]
gssexam$religion <- NA
gssexam$religion[gssexam$relig == "Protestant"] <- "Protestant"
gssexam$religion[gssexam$relig != "Protestant"] <- "Others"
gssexam <- read.csv("~/Desktop/gssexam.csv")
f <- gssexam %>%
select(relig) %>%
group_by(relig) %>%
count()
f$relig[which.max(f$freq)]
gssexam$religion <- NA
gssexam$religion[gssexam$relig == "Protestant"] <- "Protestant"
gssexam$religion[gssexam$relig != "Protestant"] <- "Others"
gssexam <- read.csv("~/Desktop/gssexam.csv")
f <- gssexam %>%
select(relig) %>%
group_by(relig) %>%
count()
f$relig[which.max(f$freq)]
gssexam$religion <- NA
gssexam$religion[gssexam$relig == f$relig[which.max(f$freq)]] <- f$relig[which.max(f$freq)]
gssexam$religion[gssexam$relig != f$relig[which.max(f$freq)]] <- "Others"
"Protestant"
View(f)
gssexam$religion[gssexam$relig==NA]<-"Others"
#######################
# set working directory
# load data
# and load libraries
#######################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo"), pkgTest)
##################
#b) Recode the variable *relig* to a new dichotomous variable *religion* with two categories:
#the one is the category with the most frequency named as itself,
#the other one is the rest of categories named "Others".
##################
gssexam <- read.csv("~/Desktop/gssexam.csv")
f <- gssexam %>%
select(relig) %>%
group_by(relig) %>%
count()
f$relig[which.max(f$freq)]
gssexam$religion <- NA
gssexam$religion[gssexam$relig == "Protestant"] <- "Protestant"
gssexam$religion[gssexam$relig != "Protestant"] <- "Others"
gssexam$religion[gssexam$relig==NA]<-"Others"
View(gssexam)
gssexam$religion <- NA
gssexam$religion[gssexam$relig == "Protestant"] <- "Protestant"
gssexam$religion[gssexam$relig==NA]<-"Others"
View(gssexam)
gssexam$religion[is.na(gssexam$relig)]<-"Others"
gssexam$religion <- NA
gssexam$religion[gssexam$relig == "Protestant"] <- "Protestant"
gssexam$religion[is.na(gssexam$religlion)]<-"Others"
gssexam$religion[is.na(gssexam$religlion)]
gssexam$religion <- NA
gssexam$religion[gssexam$relig == "Protestant"] <- "Protestant"
is.na(gssexam$religlion)
gssexam$religion[is.na(gssexam$religion)] <-"Others"
gssexam$religion[gssexam$religion==NA] <-"Others"
gssexam$religion[gssexam$relig == "Protestant"] <- "Protestant"
gssexam$religion <- NA
gssexam$religion[gssexam$relig == "Protestant"] <- "Protestant"
gssexam$religion[gssexam$religion==NA] <-"Others"
gssexam$religion <- NA
gssexam$religion[gssexam$relig == "Protestant"] <- "Protestant"
gssexam$religion[is.na(gssexam$religion)] <-"Others"
gssexam <- read.csv("~/Desktop/gssexam.csv")
f <- gssexam %>%
select(relig) %>%
group_by(relig) %>%
count()
f$relig[which.max(f$freq)]
gssexam$religion <- NA
gssexam$religion[gssexam$relig == "Protestant"] <- "Protestant"
gssexam$religion[is.na(gssexam$religion)] <-"Others"
View(gssexam)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
gssexam %>%
group_by(age) %>%
mutate(avgtv=mean(tvhours,na.rm=T)) %>%
ggplot(aes(x=age,y=avgtv))+
geom_point()+
labs(x="Age", y="Average Hours", title="Average hours of watch tv at each age by different races")+
facet_wrap(~race,ncol=2)
#####################
# load libraries
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower")
########################
# download all csv files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_csv <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".csv", collapse = "")
# download file
# tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
download_csv("commonswrittenquestions", 510, "Written_Questions")
#####################
# load libraries
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower")
########################
# download all csv files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_csv <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
browse()
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".csv", collapse = "")
# download file
# tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
download_csv("commonswrittenquestions", 510, "Written_Questions")
#####################
# load libraries
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower")
########################
# download all csv files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_csv <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
url
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".csv", collapse = "")
# download file
# tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
download_csv("commonswrittenquestions", 510, "Written_Questions")
#####################
# load libraries
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower")
########################
# download all csv files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_csv <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
url
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".csv", collapse = "")
# download file
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
download_csv("commonswrittenquestions", 510, "Written_Questions")
#####################
# load libraries
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower")
########################
# download all csv files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_csv <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
url
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".csv", collapse = "")
# download file
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
download_csv("commonswrittenquestions", 510, "Written_Questions")
commonswrittenquestions_page_1 <- read.csv("~/Documents/GitHub/CompLegFall2019/data/uk_lower/Written_Questions/commonswrittenquestions_page_1.csv")
View(commonswrittenquestions_page_1)
#####################
# load libraries
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower")
########################
# download all csv files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_csv <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
url
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".csv", collapse = "")
# download file
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
download_csv("commonswrittenquestions", 510, "Written_Questions")
#####################
# load libraries
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower")
########################
# download all csv files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_csv <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
url
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".csv", collapse = "")
# download file
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
download_csv("commonswrittenquestions", 510, "Written_Questions")
