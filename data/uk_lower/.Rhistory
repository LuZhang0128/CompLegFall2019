new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
# working directoy
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/")
# need to iterate over different criteria to download
# first, select that characteristics we're interested in
MPcharacteristics <- c("Committees", "OppositionPosts", "GovernmentPosts"
, "Constituencies"
)
# indicate the base URL to include all members from Commons
baseURL <- "http://data.parliament.uk/membersdataplatform/services/mnis/members/query/House=Commons|Membership=all/"
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
# working directoy
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/")
####################
# Download XML files
####################
# need to iterate over different criteria to download
# first, select that characteristics we're interested in
MPcharacteristics <- c("Committees", "OppositionPosts", "GovernmentPosts"
, "Constituencies"
)
# indicate the base URL to include all members from Commons
baseURL <- "http://data.parliament.uk/membersdataplatform/services/mnis/members/query/House=Commons|Membership=all/"
for(i in seq(1, length(MPcharacteristics), by=3)){
browser()
# set url
url <- str_c(baseURL, MPcharacteristics[i], "|", MPcharacteristics[i+1], "|", MPcharacteristics[i+2], collapse = "")
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".xml", collapse = "")
# download xml using url
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
# set url
url <- str_c(baseURL, MPcharacteristics[i], "|", MPcharacteristics[i+1], "|", MPcharacteristics[i+2], collapse = "")
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".xml", collapse = "")
url
getwd()
str_c(getwd(), "/", raw_characteristics, "_", i, ".xml", collapse = "")
str_c(getwd(), "/raw_characteristics_", i, ".xml", collapse = "")
str_c(getwd(), "/raw_characteristics/", i, ".xml", collapse = "")
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
# working directoy
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/")
####################
# Download XML files
####################
# need to iterate over different criteria to download
# first, select that characteristics we're interested in
MPcharacteristics <- c("Committees", "OppositionPosts", "GovernmentPosts"
, "Constituencies"
)
# indicate the base URL to include all members from Commons
baseURL <- "http://data.parliament.uk/membersdataplatform/services/mnis/members/query/House=Commons|Membership=all/"
for(i in seq(1, length(MPcharacteristics), by=3)){
# set url
url <- str_c(baseURL, MPcharacteristics[i], "|", MPcharacteristics[i+1], "|", MPcharacteristics[i+2], collapse = "")
# make file name
file <- str_c(getwd(), "/raw_characteristics/", i, ".xml", collapse = "")
# download xml using url
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
#
#
# datasets <- data.frame(type=c(), maxPages=c(), fileName=c())
#
# for(set in 1:length(unique(datasets$type))){
#   download_csv(type = datasets[set, "type"], maxPages = datasets[set, "maxPages"],
#                fileName = datasets[set, "fileName"])
# }
for(i in seq(1, length(MPcharacteristics), by=3)){
# set url
url <- str_c(baseURL, MPcharacteristics[i], "|", MPcharacteristics[i+1], "|", MPcharacteristics[i+2], collapse = "")
browser()
# make file name
file <- str_c(getwd(), "/raw_characteristics/", i, ".xml", collapse = "")
# download xml using url
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
i=4
# set url
url <- str_c(baseURL, MPcharacteristics[i], "|", MPcharacteristics[i+1], "|", MPcharacteristics[i+2], collapse = "")
url
MPcharacteristics[i]
# set url
if(is.null(MPcharacteristics[i+1] | MPcharacteristics[i+2]))
)
# set url
if(is.null(MPcharacteristics[i+1] | MPcharacteristics[i+2]))
# set url
is.null(MPcharacteristics[i+1] | MPcharacteristics[i+2])
# set url
is.null(MPcharacteristics[i+1])
MPcharacteristics[i+1]
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
# working directoy
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/")
####################
# Download XML files
####################
# need to iterate over different criteria to download
# first, select that characteristics we're interested in
MPcharacteristics <- c("Committees", "OppositionPosts", "GovernmentPosts"
, "Constituencies"
)
# indicate the base URL to include all members from Commons
baseURL <- "http://data.parliament.uk/membersdataplatform/services/mnis/members/query/House=Commons|Membership=all/"
for(i in seq(1, length(MPcharacteristics), by=3)){
# set url
if(is.na(MPcharacteristics[i+1])){
url <- str_c(baseURL, MPcharacteristics[i], collapse = "")
}
if(is.na(MPcharacteristics[i+2])){
url <- str_c(baseURL, MPcharacteristics[i], "|", MPcharacteristics[i+1], collapse = "")
}
url <- str_c(baseURL, MPcharacteristics[i], "|", MPcharacteristics[i+1], "|", MPcharacteristics[i+2], collapse = "")
# make file name
file <- str_c(getwd(), "/raw_characteristics/", i, ".xml", collapse = "")
# download xml using url
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
#
#
# datasets <- data.frame(type=c(), maxPages=c(), fileName=c())
#
# for(set in 1:length(unique(datasets$type))){
#   download_csv(type = datasets[set, "type"], maxPages = datasets[set, "maxPages"],
#                fileName = datasets[set, "fileName"])
# }
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
# working directoy
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/")
####################
# Download XML files
####################
# need to iterate over different criteria to download
# first, select that characteristics we're interested in
MPcharacteristics <- c("Committees", "OppositionPosts", "GovernmentPosts"
, "Constituencies"
)
# indicate the base URL to include all members from Commons
baseURL <- "http://data.parliament.uk/membersdataplatform/services/mnis/members/query/House=Commons|Membership=all/"
for(i in 1:length(MPcharacteristics)){
# set url
url <- str_c(baseURL, MPcharacteristics[i], collapse = "")
browser()
# make file name
file <- str_c(getwd(), "/raw_characteristics/", i, ".xml", collapse = "")
# download xml using url
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
#
#
# datasets <- data.frame(type=c(), maxPages=c(), fileName=c())
#
# for(set in 1:length(unique(datasets$type))){
#   download_csv(type = datasets[set, "type"], maxPages = datasets[set, "maxPages"],
#                fileName = datasets[set, "fileName"])
# }
url
Q
for(i in 1:length(MPcharacteristics)){
# set url
url <- str_c(baseURL, MPcharacteristics[i], collapse = "")
browser()
# make file name
file <- str_c(getwd(), "/raw_characteristics/", i, ".xml", collapse = "")
# download xml using url
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
str_c(getwd(), "/raw_characteristics/", i, ".xml", collapse = "")
str_c(getwd(), "/raw_characteristics/", MPcharacteristics[i], ".xml", collapse = "")
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
# working directoy
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/")
####################
# Download XML files
####################
# need to iterate over different criteria to download
# first, select that characteristics we're interested in
MPcharacteristics <- c("Committees", "OppositionPosts", "GovernmentPosts"
, "Constituencies"
)
# indicate the base URL to include all members from Commons
baseURL <- "http://data.parliament.uk/membersdataplatform/services/mnis/members/query/House=Commons|Membership=all/"
for(i in 1:length(MPcharacteristics)){
# set url
url <- str_c(baseURL, MPcharacteristics[i], collapse = "")
# make file name
file <- str_c(getwd(), "/raw_characteristics/", MPcharacteristics[i], ".xml", collapse = "")
# download xml using url
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
#
#
# datasets <- data.frame(type=c(), maxPages=c(), fileName=c())
#
# for(set in 1:length(unique(datasets$type))){
#   download_csv(type = datasets[set, "type"], maxPages = datasets[set, "maxPages"],
#                fileName = datasets[set, "fileName"])
# }
Q
Q
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
# working directoy
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/")
####################
# Download XML files
####################
# need to iterate over different criteria to download
# first, select that characteristics we're interested in
MPcharacteristics <- c("Committees", "OppositionPosts", "GovernmentPosts"
, "Constituencies"
)
# indicate the base URL to include all members from Commons
baseURL <- "http://data.parliament.uk/membersdataplatform/services/mnis/members/query/House=Commons|Membership=all/"
for(i in 1:length(MPcharacteristics)){
# set url
url <- str_c(baseURL, MPcharacteristics[i], collapse = "")
# make file name
browser()
file <- str_c(getwd(), "/raw_characteristics/", MPcharacteristics[i], ".xml", collapse = "")
# download xml using url
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
#
#
# datasets <- data.frame(type=c(), maxPages=c(), fileName=c())
#
# for(set in 1:length(unique(datasets$type))){
#   download_csv(type = datasets[set, "type"], maxPages = datasets[set, "maxPages"],
#                fileName = datasets[set, "fileName"])
# }
str_c(getwd(), "/raw_characteristics/", MPcharacteristics[i], ".xml", collapse = "")
for(i in 1:length(MPcharacteristics)){
# set url
url <- str_c(baseURL, MPcharacteristics[i], collapse = "")
# make file name
browser()
file <- str_c(getwd(), "/raw_characteristics/", MPcharacteristics[i], ".xml", collapse = "")
# download xml using url
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
url
str_c(getwd(), "/raw_characteristics/", MPcharacteristics[i], ".xml", collapse = "")
file <- str_c(getwd(), "/", MPcharacteristics[i], "/", MPcharacteristics[i], "raw_characteristics.xml", collapse = "")
file
file <- str_c(getwd(), "/", MPcharacteristics[i], "/", MPcharacteristics[i], "_raw_characteristics.xml", collapse = "")
file
file <- str_c(getwd(), "/raw_characteristics/", MPcharacteristics[i], "_raw_characteristics.xml", collapse = "")
file
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
# working directoy
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/")
####################
# Download XML files
####################
# need to iterate over different criteria to download
# first, select that characteristics we're interested in
MPcharacteristics <- c("Committees", "OppositionPosts", "GovernmentPosts"
, "Constituencies"
)
# indicate the base URL to include all members from Commons
baseURL <- "http://data.parliament.uk/membersdataplatform/services/mnis/members/query/House=Commons|Membership=all/"
for(i in 1:length(MPcharacteristics)){
# set url
url <- str_c(baseURL, MPcharacteristics[i], collapse = "")
# make file name
file <- str_c(getwd(), "/raw_characteristics/", MPcharacteristics[i], "_raw_characteristics.xml", collapse = "")
# download xml using url
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
#
#
# datasets <- data.frame(type=c(), maxPages=c(), fileName=c())
#
# for(set in 1:length(unique(datasets$type))){
#   download_csv(type = datasets[set, "type"], maxPages = datasets[set, "maxPages"],
#                fileName = datasets[set, "fileName"])
# }
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
# working directoy
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/")
####################
# Download XML files
####################
# need to iterate over different criteria to download
# first, select that characteristics we're interested in
MPcharacteristics <- c("Committees", "OppositionPosts", "GovernmentPosts", "Constituencies",
"Parties", "Experiences", "Interests", "ParliamentaryPosts", "Staff"
)
# indicate the base URL to include all members from Commons
baseURL <- "http://data.parliament.uk/membersdataplatform/services/mnis/members/query/House=Commons|Membership=all/"
for(i in 1:length(MPcharacteristics)){
# set url
url <- str_c(baseURL, MPcharacteristics[i], collapse = "")
# make file name
file <- str_c(getwd(), "/raw_characteristics/", MPcharacteristics[i], "_raw_characteristics.xml", collapse = "")
# download xml using url
tryCatch(download.file(url, file, quiet = TRUE), error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
#
#
# datasets <- data.frame(type=c(), maxPages=c(), fileName=c())
#
# for(set in 1:length(unique(datasets$type))){
#   download_csv(type = datasets[set, "type"], maxPages = datasets[set, "maxPages"],
#                fileName = datasets[set, "fileName"])
# }
