########################
# download all XML files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_xml <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".xml", collapse = "")
# download file
tryCatch(download.file(url, file, quiet = TRUE),
error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
datasets <- data.frame(type=c("constituencies"), maxPages=c(7), fileName=c("Constituencies"))
# Last, execute function for each dataset you download
for(set in 1:length(unique(datasets$type))){
download_xml(type = datasets[set, "type"], maxPages = datasets[set, "maxPages"],
fileName = datasets[set, "fileName"])
}
warnings()
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/Constituencies/data")
#####################
# load libraries
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/Constituencies/data")
########################
# download all XML files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_xml <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".xml", collapse = "")
# download file
tryCatch(download.file(url, file, quiet = TRUE),
error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
download_xml("constituencies", 7, "Constituencies")
}
#####################
# load libraries
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/Constituencies/data")
########################
# download all XML files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_xml <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".xml", collapse = "")
# download file
tryCatch(download.file(url, file, quiet = TRUE),
error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
download_xml("constituencies", 7, "Constituencies")
#####################
# load libraries
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower")
########################
# download all XML files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_xml <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".xml", collapse = "")
# download file
tryCatch(download.file(url, file, quiet = TRUE),
error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
download_xml("constituencies", 7, "Constituencies")
#####################
# load libraries
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower")
########################
# download all XML files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_xml <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".csv", collapse = "")
# download file
tryCatch(download.file(url, file, quiet = TRUE),
error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
download_xml("constituencies", 7, "Constituencies")
do.call_rbind_read.csv('~/Documents/GitHub/CompLegFall2019/data/uk_lower/Constituencies')
do.call_rbind_read.csv <- function(path, pattern = "*.csv") {
files = list.files(path, pattern, full.names = TRUE)
do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
}
do.call_rbind_read.csv('~/Documents/GitHub/CompLegFall2019/data/uk_lower/Constituencies')
df <- do.call_rbind_read.csv('~/Documents/GitHub/CompLegFall2019/data/uk_lower/Constituencies')
View(df)
View(df)
read.csv("constituencies_page_0.csv")
setwd('~/Documents/GitHub/CompLegFall2019/data/uk_lower/Constituencies')
read.csv("constituencies_page_0.csv")
a <- read.csv("constituencies_page_0.csv")
View(df)
View(df)
###########################
# parse the dataset
###########################
df_a <- df %>%
select(label) %>%
unique()
View(df)
df_a$constituency_name <- df %>%
select(label) %>%
unique()
do.call_rbind_read.csv <- function(path, pattern = "*.csv") {
files = list.files(path, pattern, full.names = TRUE)
do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
}
df <- do.call_rbind_read.csv('~/Documents/GitHub/CompLegFall2019/data/uk_lower/Constituencies')
clear
do.call_rbind_read.csv <- function(path, pattern = "*.csv") {
files = list.files(path, pattern, full.names = TRUE)
do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
}
df <- do.call_rbind_read.csv('~/Documents/GitHub/CompLegFall2019/data/uk_lower/Constituencies')
df_a$constituency_name <- df %>%
select(label) %>%
unique()
###########################
# parse the dataset
###########################
# we only need the name of constituency
df_a <- df %>%
select(label) %>%
unique()
# we only need the name of constituency from the original dataset
# it is the same as province name
df_a <- df %>%
select(label) %>%
unique()
colnames(df_a) <- c("constituency_name")
df_a$province_name <- df_a$constituency_name
# One Member of Parliament (MP) in the House of Commons represents a single constituency.
df_a$chamber_number <- 1
df_a$chamber_name <- "House o
df_a <- df %>%
select(label) %>%
unique()
colnames(df_a) <- c("constituency_name")
df_a$province_name <- df_a$constituency_name
# One Member of Parliament (MP) in the House of Commons represents a single constituency.
df_a$chamber_number <- 1
df_a$chamber_name <- "House of Commons"
View(df_a)
df_a <- df %>%
select(label) %>%
unique()
colnames(df_a) <- c("constituency_name")
df_a$province_name <- df_a$constituency_name
# One Member of Parliament (MP) in the House of Commons represents a single constituency.
df_a$chamber_number <- 1
df_a$chamber_name <- "House of Commons"
# A number assigned to each chamber. Assigned with constituencies sorted by name.
df_b <- df_a[order("constituency_name"),]
View(df_b)
# A number assigned to each chamber. Assigned with constituencies sorted by name.
df_b <- df_a[order("constituency_name")]
# A number assigned to each chamber. Assigned with constituencies sorted by name.
df_b <- df_a[order("constituency_name"), ]
df_a <- df %>%
select(label) %>%
unique()
colnames(df_a) <- c("constituency_name")
df_a$province_name <- df_a$constituency_name
# One Member of Parliament (MP) in the House of Commons represents a single constituency.
df_a$chamber_number <- 1
df_a$chamber_name <- "House of Commons"
# A number assigned to each chamber. Assigned with constituencies sorted by name.
df_a <- df_a[order("constituency_name"), ]
df_a <- df %>%
select(label) %>%
unique()
colnames(df_a) <- c("constituency_name")
df_a$province_name <- df_a$constituency_name
# One Member of Parliament (MP) in the House of Commons represents a single constituency.
df_a$chamber_number <- 1
df_a$chamber_name <- "House of Commons"
# A number assigned to each chamber. Assigned with constituencies sorted by name.
df_b <- df_a[order("constituency_name"), ]
View(df_a)
# A number assigned to each chamber. Assigned with constituencies sorted by name.
df_b <- df_a[order(df_a$constituency_name), ]
View(df_b)
df_b$constituency_number <- rownames(df_b)
View(df_b)
# observation number is based on chamber number and constituency number
df_c <- df_b[order(df_b$chamber_number, df_b$constituency_number), ]
View(df_c)
View(df_b)
View(df_c)
# observation number is based on chamber number and constituency number
df_c <- df_b[order(as.numeric(df_b$chamber_number), as.numeric(df_b$constituency_number)), ]
###########################
# parse the dataset
###########################
# we only need the name of constituency from the original dataset
# it is the same as province name
df_a <- df %>%
select(label) %>%
unique()
colnames(df_a) <- c("constituency_name")
df_a$province_name <- df_a$constituency_name
# One Member of Parliament (MP) in the House of Commons represents a single constituency.
df_a$chamber_number <- 1
df_a$chamber_name <- "House of Commons"
# A number assigned to each chamber. Assigned with constituencies sorted by name.
df_b <- df_a[order(df_a$constituency_name), ]
df_b$constituency_number <- rownames(df_b)
# observation number is based on chamber number and constituency number
df_c <- df_b[order(as.numeric(df_b$chamber_number), as.numeric(df_b$constituency_number)), ]
View(df_c)
df_a <- df %>%
select(label) %>%
unique()
colnames(df_a) <- c("constituency_name")
df_a$province_name <- df_a$constituency_name
# One Member of Parliament (MP) in the House of Commons represents a single constituency.
df_a$chamber_number <- 1
df_a$chamber_name <- "House of Commons"
# A number assigned to each chamber. Assigned with constituencies sorted by name.
df_b <- df_a[order(df_a$constituency_name), ]
df_b$constituency_number <- rownames(df_b)
# observation number is based on chamber number and constituency number
View(df_a)
View(df_b)
# observation number is based on chamber number and constituency number
df_b$observation_number <- rownames(df_b)
View(df_b)
df_b <- df_a[order(df_a$constituency_name), ]
rownames(df_b) <- c()
df_b$constituency_number <- rownames(df_b)
View(df_b)
# observation number is based on chamber number and constituency number
df_c <- df_b[order(as.numeric(df_b$chamber_number), as.numeric(df_b$constituency_number)), ]
View(df_c)
df_a <- df %>%
select(label) %>%
unique()
colnames(df_a) <- c("constituency_name")
df_a$province_name <- df_a$constituency_name
# One Member of Parliament (MP) in the House of Commons represents a single constituency.
df_a$chamber_number <- 1
df_a$chamber_name <- "House of Commons"
# A number assigned to each chamber. Assigned with constituencies sorted by name.
df_b <- df_a[order(df_a$constituency_name), ]
rownames(df_b) <- c()
df_b$constituency_number <- rownames(df_b)
# observation number is based on chamber number and constituency number
df_c <- df_b[order(as.numeric(df_b$chamber_number), as.numeric(df_b$constituency_number)), ]
rownames(df_c) <- c()
df_c$observation_number <- rownames(df_c)
# set pathes
df_c$chamber_path <- paste("/chamber-",df_c$chamber_number,sep="")
View(df_c)
df_c$constituency_path <- paste(df_g$chamber_path,"/constituency-",df_c$constituency_number,sep="")
df_c$constituency_path <- paste(df_c$chamber_path,"/constituency-",df_c$constituency_number,sep="")
df_c$observation_path <- paste(df_c$chamber_path,"/constituency-",df_c$observation_number,sep="")
colnames(df_c)
#####################
# load libraries
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
#######################
# working directoy
#######################
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower")
########################
# download all csv files
########################
# create function that takes inputs of:
# (1) type: what data base do you want to access (bills, divisions, etc.)?
# (2) maxPages: this is the number of web pages the documents are stored across
# the API only lets you download 500 at a time, so you have to iterate over pages
# (3) fileName: where is the destination of the file?
# Our labelling is slightly different than the website (ex: "answeredquestions" & "Written_Responses")
download_csv <- function(type, maxPages, fileName){
for(i in 0:maxPages) {
# make URL
url <- str_c("http://lda.data.parliament.uk/", type,".csv?_pageSize=500&_page=", i, collapse = "")
# make file name
file <- str_c(getwd(), "/", fileName, "/", type, "_page_", i, ".csv", collapse = "")
# download file
tryCatch(download.file(url, file, quiet = TRUE),
error = function(e) print(paste(file, 'questions missing')))
# random delay
Sys.sleep(runif(1, 0, 0.15))
}
}
download_csv("constituencies", 7, "Constituencies")
############################
# read the download csv
# bind them together
############################
do.call_rbind_read.csv <- function(path, pattern = "*.csv") {
files = list.files(path, pattern, full.names = TRUE)
do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
}
df <- do.call_rbind_read.csv('~/Documents/GitHub/CompLegFall2019/data/uk_lower/Constituencies')
###########################
# parse the dataset
###########################
# we only need the name of constituency from the original dataset
# it is the same as province name
df_a <- df %>%
select(label) %>%
unique()
colnames(df_a) <- c("constituency_name")
df_a$province_name <- df_a$constituency_name
# One Member of Parliament (MP) in the House of Commons represents a single constituency.
df_a$chamber_number <- 1
df_a$chamber_name <- "House of Commons"
# A number assigned to each chamber. Assigned with constituencies sorted by name.
df_b <- df_a[order(df_a$constituency_name), ]
rownames(df_b) <- c()
df_b$constituency_number <- rownames(df_b)
# observation number is based on chamber number and constituency number
df_c <- df_b[order(as.numeric(df_b$chamber_number), as.numeric(df_b$constituency_number)), ]
rownames(df_c) <- c()
df_c$observation_number <- rownames(df_c)
# set pathes
df_c$chamber_path <- paste("/chamber-",df_c$chamber_number,sep="")
df_c$constituency_path <- paste(df_c$chamber_path,"/constituency-",df_c$constituency_number,sep="")
df_c$observation_path <- paste(df_c$chamber_path,"/constituency-",df_c$observation_number,sep="")
colnames(df_c)
# re-arrange and output
uk_lower_constituencies <- df_c %>%
select(observation_path,
chamber_path,
constituency_path,
observation_number,
chamber_number,
chamber_name,
constituency_number,
constituency_name,
province_name)
###############
# write csv
###############
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/Constituencies")
write.csv(uk_lower_constituencies, "uk_lower_constituencies.csv")
View(df)
View(df_c)
View(uk_lower_constituencies)
