<<<<<<< HEAD
return(count_df)
}
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "paragraph_text"]))
}
# find unique words in word matrix
unique(all_words$word)
# there are 676 unique words in corpus
###################################
# create open DTM filled w/ zeroes
###################################
DTM <- matrix(0, nrow=dim(speechesDF)[1], ncol=length(unique(all_words$word)))
# assign column names of DTM to be the unique words (in alpha order)
colnames(DTM) <- unique(all_words$word)
# loop over each "document"/paragraph
for(document in 1:dim(speechesDF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
DTM[document, which(colnames(DTM)==document_subset[row, "word"] )] <- all_words[row, "count"]
}
}
# compare how we did
# first look at DTM for 50th observation
which(DTM[50,]>0)
speechesDF[50,"paragraph_text"]
##################################
# Doing this all using library(tm)
##################################
# create new corpus object with tm
speech_corpus <- Corpus(VectorSource(speechesDF$paragraph_text))
# clean corpus as we did before
speech_corpus <- tm_map(speech_corpus, content_transformer(tolower))
speech_corpus <- tm_map(speech_corpus, removeNumbers)
speech_corpus <- tm_map(speech_corpus, removePunctuation)
speech_corpus <- tm_map(speech_corpus, removeWords, c("the", "and", stopwords("english")))
speech_corpus <- tm_map(speech_corpus, stripWhitespace)
# create DTM
alternative_DTM <- DocumentTermMatrix(speech_corpus)
View(DTM)
View(speechesDF)
# 2019/11/08
n = freq(DTM)
# 2019/11/08
n = sum(DTM[])/length(DTM[])
# 2019/11/08
n = sum(DTM)/length(DTM)
# 2019/11/08{
for i in 1:nrows(DTM){
f[i] = sum(DTM[i])/length(DTM[i])
}
# 2019/11/08{
for (i in 1:nrows(DTM)){
f[i] = sum(DTM[i])/length(DTM[i])
}
# 2019/11/08{
for (i in 1:nrow(DTM)){
f[i] = sum(DTM[i])/length(DTM[i])
}
# 2019/11/08{
f = 0;
for (i in 1:nrow(DTM)){
f[i] = sum(DTM[i])/length(DTM[i])
}
View(DTM)
View(speech_corpus)
View(speechesDF)
# 2019/11/08{
f = colSums(DTM)/length(DTM)
# 2019/11/08{
idf = colSums(DTM)/length(DTM)
View(idf)
IDF = length(DTM)/colSums(DTM)
IDF = 1/TF
# 2019/11/08{
TF = colSums(DTM)/length(DTM)
IDF = 1/TF
=======
>>>>>>> 3c41c2b545e982c5916d84e182495035bd5d7a75
#######################
# set working directory
# load data
# and load libraries
#######################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# Jeff wd
setwd('~/Documents/GitHub/CompLegFall2019/data/canadaTextParsing/Examples/')
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("quanteda", "stringr", "tm"), pkgTest)
# load speeches data from 38th parliament
speechesDF <- read.csv("canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
###################################
<<<<<<< HEAD
# create function to clean speeches
# to create DTM
###################################
=======
>>>>>>> 3c41c2b545e982c5916d84e182495035bd5d7a75
clean_text <- function(inputVec){
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "paragraph_text"]))
}
# find unique words in word matrix
unique(all_words$word)
<<<<<<< HEAD
# there are 676 unique words in corpus
###################################
# create open DTM filled w/ zeroes
###################################
DTM <- matrix(0, nrow=dim(speechesDF)[1], ncol=length(unique(all_words$word)))
# assign column names of DTM to be the unique words (in alpha order)
colnames(DTM) <- unique(all_words$word)
# loop over each "document"/paragraph
for(document in 1:dim(speechesDF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
DTM[document, which(colnames(DTM)==document_subset[row, "word"] )] <- all_words[row, "count"]
}
}
# compare how we did
# first look at DTM for 50th observation
which(DTM[50,]>0)
speechesDF[50,"paragraph_text"]
##################################
# Doing this all using library(tm)
##################################
# create new corpus object with tm
speech_corpus <- Corpus(VectorSource(speechesDF$paragraph_text))
# clean corpus as we did before
speech_corpus <- tm_map(speech_corpus, content_transformer(tolower))
speech_corpus <- tm_map(speech_corpus, removeNumbers)
speech_corpus <- tm_map(speech_corpus, removePunctuation)
speech_corpus <- tm_map(speech_corpus, removeWords, c("the", "and", stopwords("english")))
speech_corpus <- tm_map(speech_corpus, stripWhitespace)
# create DTM
alternative_DTM <- DocumentTermMatrix(speech_corpus)
# check to see what it looks like
inspect(alternative_DTM[1:50, 1:50])
norm_vec <- function(x) {
sqrt(sum(x^2))
}
euclidean <- function(x,y) {
sqrt(sum((x-y)^2))
}
cosine <- function(x,y) {
(x/norm_vec(x)) %*% (y/norm_vec(y))
}
x <- c(1,0,1)
y <- c(1/2,0,1/2)
euclidean(x,y)
cosine(x,y)
# 2019/11/08{
TF = colSums(DTM)/length(DTM)
IDF = 1/TF
Distance_Matrix = dist(IDF, method = "eudlidean", diag=F)
Distance_Matrix = dist(IDF, method = "eudlidean")
IDF = log(1/TF)
Distance_Matrix = dist(diag(IDF), method = "eudlidean")
?dist()
Distance_Matrix = dist(diag(IDF), method = "euclidean")
IDF_matrix = diag(IDF)
View(IDF_matrix)
#######################
# set working directory
# load data
# and load libraries
#######################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# Jeff wd
setwd('~/Documents/GitHub/CompLegFall2019/data/canadaTextParsing/Examples/')
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("quanteda", "stringr", "tm"), pkgTest)
# load speeches data from 38th parliament
speechesDF <- read.csv("canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
###################################
# create function to clean speeches
# to create DTM
###################################
clean_text <- function(inputVec){
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "paragraph_text"]))
}
# find unique words in word matrix
unique(all_words$word)
# there are 676 unique words in corpus
###################################
# create open DTM filled w/ zeroes
###################################
=======
>>>>>>> 3c41c2b545e982c5916d84e182495035bd5d7a75
DTM <- matrix(0, nrow=dim(speechesDF)[1], ncol=length(unique(all_words$word)))
# assign column names of DTM to be the unique words (in alpha order)
colnames(DTM) <- unique(all_words$word)
# loop over each "document"/paragraph
for(document in 1:dim(speechesDF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
DTM[document, which(colnames(DTM)==document_subset[row, "word"] )] <- all_words[row, "count"]
}
}
# compare how we did
# first look at DTM for 50th observation
which(DTM[50,]>0)
speechesDF[50,"paragraph_text"]
<<<<<<< HEAD
##################################
# Doing this all using library(tm)
##################################
# create new corpus object with tm
speech_corpus <- Corpus(VectorSource(speechesDF$paragraph_text))
# clean corpus as we did before
speech_corpus <- tm_map(speech_corpus, content_transformer(tolower))
speech_corpus <- tm_map(speech_corpus, removeNumbers)
speech_corpus <- tm_map(speech_corpus, removePunctuation)
speech_corpus <- tm_map(speech_corpus, removeWords, c("the", "and", stopwords("english")))
speech_corpus <- tm_map(speech_corpus, stripWhitespace)
# create DTM
alternative_DTM <- DocumentTermMatrix(speech_corpus)
# check to see what it looks like
inspect(alternative_DTM[1:50, 1:50])
norm_vec <- function(x) {
sqrt(sum(x^2))
}
euclidean <- function(x,y) {
sqrt(sum((x-y)^2))
}
cosine <- function(x,y) {
(x/norm_vec(x)) %*% (y/norm_vec(y))
}
x <- c(1,0,1)
y <- c(1/2,0,1/2)
euclidean(x,y)
cosine(x,y)
# 2019/11/08{
TF = colSums(DTM)/nrow(DTM)
IDF = log(1/TF)
IDF_matrix = diag(IDF)
Distance_matrix = dist(IDF_matrix, method = "euclidean")
Distance_matrix = TF*IDF
View(IDF_matrix)
TF = colSums(DTM)/nrow(DTM)
IDF = log(1/TF)
IDF_matrix = diag(IDF)
Distance_matrix = dist(IDF_matrix, method = "euclidean")
View(Distance_matrix)
IDF = log(ncol(TF) / ( 1+rowSums(TF != 0))))
IDF = log(ncol(TF) / ( 1+rowSums(TF != 0)))
IDF = log(ncol(DTM) / ( 1+rowSums(DTM != 0)))
View(DTM)
IDF = log(1/TF)
IDF = log(ncol(DTM) / ( 1+rowSums(DTM != 0)))
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
TF = colSums(DTM)/nrow(DTM)
IDF = log(1/TF)
TF_IDF = TF*IDF
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
TF_IDF = TF*IDF
IDF = log(nrow(DTM)/TF)
TF_IDF = TF*IDF
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
IDF = log(nrow(DTM)/colSums(DTM))
TF_IDF = TF*IDF
TF_IDF = t(TF)*IDF
View(speechesDF)
View(TF_IDF)
View(IDF)
TF_IDF = IDF*TF
View(IDF_matrix)
nrow(DTM)
colSums(DTM)
rowSums(DTM)
IDF = log(nrow(DTM)/rowSums(DTM))
IDF
?crossprod
TF = colSums(DTM)/nrow(DTM)
IDF = log(nrow(DTM)/rowSums(DTM))
TF_IDF = crossprod(TF, IDF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
TF = colSums(DTM)/nrow(DTM)
IDF = log(nrow(DTM)/rowSums(DTM))
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF = colSums(DTM)/nrow(DTM)
IDF = log(nrow(DTM)/rowSums(DTM))
TF_IDF = crossprod(TF, IDF)
# 2019/11/08
TF = rowSums(DTM)/nrow(DTM)
IDF = log(nrow(DTM)/rowSums(DTM))
TF_IDF = crossprod(TF, IDF)
View(DTM)
# 2019/11/08
TF = colSums(DTM)/nrow(DTM)
IDF = log(nrow(DTM)/colSums(DTM))
TF_IDF = crossprod(TF, IDF)
colSums(DTM)
# 2019/11/08
sum(colSums(DTM)==0)
DTM
IDF = log(nrow(DTM)/colSums(DTM))
IDF
idf
IDF = log(nrow(DTM)/colSums(DTM))
IDF <- diag(IDF)
TF_IDF = crossprod(DTM, IDF)
DTM
IDF
TF_IDF = crossprod(DTM, IDF)
str(DTM)
TF_IDF = crossprod(as.matrix(DTM), IDF)
IDF
dim(IDF)
dim(TF)
dim(DTM)
TF_IDF = crossprod(t(DTM), IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
TF_IDF_normalized
head(TF_IDF_normalized)
IDF = log(ncol(DTM)/rowSums(DTM))
TF <- t(DTM)
IDF = log(ncol(DTM)/rowSums(DTM))
TF_IDF = crossprod(DTM, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
View(TF_IDF_normalized)
DTM_transposed <- t(DTM)
TF = rowSums(DTM_transposed)/ncol(DTM_transposed)
IDF = log(ncol(DTM_transposed)/rowSums(DTM_transposed))
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
View(TF_IDF)
TF <- t(DTM)
IDF = log(ncol(TF)/rowSums(TF))
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
TF <- t(DTM)
IDF = log(ncol(TF)/(1+rowSums(TF)!=0))
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
TF <- t(DTM)
IDF <- log(ncol(TF)/(1+rowSums(TF)!=0))
IDF <- duag(IDF)
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
# TF-IDF
TF <- t(DTM)
IDF <- log(ncol(TF)/(1+rowSums(TF)!=0))
IDF <- diag(IDF)
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
View(TF_IDF_normalized)
IDF <- log(ncol(TF)/(rowSums(TF)))
IDF <- diag(IDF)
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
View(TF_IDF_normalized)
# TF-IDF
TF <- t(DTM)
IDF <- log(ncol(TF)/(1+rowSums(TF)))
IDF <- diag(IDF)
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
View(TF_IDF_normalized)
# TF-IDF
TF <- t(DTM)
IDF <- log(ncol(TF)/(rowSums(TF)))
IDF <- diag(IDF)
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
View(TF_IDF_normalized)
TF <- t(DTM)
IDF <- log(ncol(TF)/(rowSums(TF)))
IDF <- diag(IDF)
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
View(TF_IDF_normalized)
IDF <- log(ncol(TF)/(1+rowSums(TF)))
IDF <- diag(IDF)
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
View(TF_IDF_normalized)
TF <- t(DTM)
IDF <- log(ncol(TF)/(rowSums(TF)))
IDF <- diag(IDF)
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
View(TF_IDF_normalized)
# TF-IDF
TF <- t(DTM)
IDF <- log(ncol(TF)/(1+rowSums(TF)))
IDF <- diag(IDF)
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
TF_IDF_normalized = TF_IDF/sqrt(rowSums(TF_IDF^2))
View(TF_IDF_normalized)
# distance matrix
Distance_matrix = dist(TF_IDF_normalized, method="euclidean")
# normalize TF-IDF
TF_IDF_normalized <- TF_IDF/sqrt(rowSums(TF_IDF^2))
sum(colSums(DTM)==0)
View(TF)
IDF <- log(ncol(TF)/(rowSums(TF)))
IDF <- diag(IDF)
View(IDF)
View(TF_IDF)
colnames(TF_IDF) <- rownames(TF)
View(TF)
=======
DTM[50,]
tabel9which(DTM[50,]>0)
table(which(DTM[50,]>0)
)
View(all_words)
>>>>>>> 3c41c2b545e982c5916d84e182495035bd5d7a75
